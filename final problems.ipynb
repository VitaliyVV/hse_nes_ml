{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "*Екатерина Лобачева / Илья Щуров / Сергей Сметанин *\n",
    "\n",
    "*Совместный бакалавриат НИУ ВШЭ и РЭШ, 2016-17 учебный год*\n",
    "\n",
    "[Страница курса](http://math-info.hse.ru/2016-17/Machine_Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задачи к итоговой контрольной работе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1\n",
    "Рассмотрим задачу классификации для двух классов. Доказать, что если классы линейно разделимы (то есть существует гиперплоскость, которая их разделяет), то задача максимизации правдоподобия в логистической регрессии имеет следующее решение: выбрать вектор весов $w$, такой, что гиперплоскость $w^Tx=0$ разделяет классы, затем устремить длину вектора $w$ к бесконечности. Каким образом можно обойти проблему, связанную со стремлением к бесконечности весов в этом случае?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2\n",
    "Рассмотрим задачу классификации для двух классов. Пусть на достаточно большой тестовой выборке классификатор $a_1$ ошибается в 90% случаев, а классификатор $a_2$ — в 50% случаев. Объяснить, почему классификатор $a_1$ может быть более полезным, чем $a_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3\n",
    "Рассмотрим задачу линеной регрессии в одномерном пространстве. Обучающая выборка состоит из точек $\\{(x_i, y_i)\\}_{i=1}^N$, $x_i\\in [-1, 1]$ — признаки, $y_i$ — ответ, который мы хотим предсказать, $\\hat y_i=kx_i+b$ — наше предсказание. \n",
    "\n",
    "На следующих картинках изображены графики ошибки $y-\\hat y$ от $x$. Какой из этих графиков мог быть получен в результате применения метода наименьших квадратов для обучения линейной регрессии? Ответ обосновать.\n",
    "\n",
    "![errors](http://math-info.hse.ru/f/2016-17/nes-ml/errors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4\n",
    "Рассмотрим набор данных со следующими признаками: рост, возраст (от 10 до 45 лет), пол (=1 для женщин, =0 для мужчин). Мы хотим построить линейную регрессионную модель для оценки высоты на основе остальных признаков. Запишите уравнение модели, учитывающей следующие факты:\n",
    "\n",
    "1. Средний рост мужчин и женщин различается.\n",
    "2. В возрасте 25 лет скорость роста очень сильно уменьшается, но мы будем продолжать считать её линейной.\n",
    "\n",
    "Ответ обосновать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5\n",
    "Рассмотрим задачу классификации для двух классов в одномерном пространстве. Пусть $X^l = \\{x_1, \\ldots, x_l\\}\\subset \\mathbb R$ — обучающая выборка, $y=\\{y_1, \\ldots y_l\\}\\subset\\{-1, 1\\}$ — вектор ответов (меток $+1$, $-1$). Доказать, что для любого вектора $y$ существует линейная композиция решающих пней (деревьев с одним предикатом), не совершающая ни одной ошибки на обучающей выборке. Композиция решающих пней действует следующим образом:\n",
    "\n",
    "$$a(x) = \\mathop{\\mathrm{sign}} \\sum_{i=1}^N \\gamma_i b_i(x),$$\n",
    "\n",
    "где $b_i$ — $i$-й решающий пень, $\\gamma_i$ — коэффициент (вещественное число) и \n",
    "$$\\mathop{\\mathrm{sign}}(z)=\n",
    "\\begin{cases}\n",
    "1,&z>0;\\\\\n",
    "0,&z=0;\\\\\n",
    "-1,&z<0.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 6\n",
    "Рассмотрим семейство алгоритмов вида $a(x)={\\mathop{\\mathrm{sign}}}(\\gamma_0 + \\gamma_1 b_1(x)+\\ldots+\\gamma_k b_k(x))$, где $b_1,\\ldots,b_k$ — решающие пни на двумерном пространстве признаков. Можно ли алгоритмом из этого семейства выделить квадрат на плоскости $\\{(x_1, x_2)\\mid 0<x_1<1, \\quad 0<x_2<1\\}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 7\n",
    "На примере нейронной сети с одним скрытым слоем, сигмоидными функциями активации и одним выходным нейроном покажите, что в начале Back Propagation нежелательно инициализировать все синаптические веса нулями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 8\n",
    "Как добавление регуляризатора (если его не было изначально) или увеличение его веса влияет на значение функционала ошибок, полученного для *обучающего* множества? Оно увеличивает ошибку, уменьшает или не меняет её?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 9\n",
    "Для какого из этих методов обучения классификаторов изменение масштаба признаков (например, нормализация) может привести к изменению ответа, а для каких не может? Ответ обосновать.\n",
    "\n",
    "1. k-nearest neighbors со стандартной евклидовой метрикой;\n",
    "2. решающее дерево;\n",
    "3. логистическая регрессия без регуляризатора;\n",
    "4. логистическая регрессия с регуляризатором;\n",
    "5. random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 10\n",
    "Для каких из следующих методов классификации граница между классами всегда будет гиперплоскостью в исходном пространстве признаков?\n",
    "\n",
    "1. k-nearest neighbors со стандартной евклидовой метрикой;\n",
    "2. решающее дерево;\n",
    "3. логистическая регрессия;\n",
    "5. нейросеть с одним скрытым слоем и сигмоидными функциями активации;\n",
    "5. random forest;\n",
    "6. boosted trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 11\n",
    "Показать, что любой классификатор $h\\colon \\{0, 1\\}^n \\to \\{0, 1\\}$ можно реализовать с помощью решающего дерева глубины $n+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 12\n",
    "Пусть $X=(x^i_j)$ — матрица, в которой строки соответствуют объектам, а столбцы признакам, причём каждый признак центрирован, то есть его среднее значение (по всем объектам) равно 0. Тогда $X^T X$ — ковариационная матрица для всех признаков. \n",
    "\n",
    "Рассмотрим задачу поиска первой главной компоненты, то есть такого направления $u$ в пространстве признаков, что разброс величин проекций $\\langle x^i, u\\rangle$ на $u$ по всем объектам $x^i$ максимальный. Её можно записать в следующем виде:\n",
    "\n",
    "$$\\|Xu\\|^2 \\to \\max_{u}$$\n",
    "$$\\|u\\|^2 =1$$\n",
    "\n",
    "Докажите, что решением этой задачи является собственный вектор, соответствующий максимальному собственному значению ковариационной матрицы $X^T X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 13\n",
    "При обучении решающих деревьев используется критерий информативности, показывающий, насколько хорошо для набора объектов $R$ их целевые переменные предсказываются константой (при оптимальном выборе этой константы):\n",
    "\n",
    "$$H(R)=\\min_{c}\\frac{1}{|R|}\\sum_{(x_i, y_i)\\in R} L(y_i, c),$$\n",
    "где $L(y, c)$ — некоторая функция потерь. Соответственно, чтобы получить вид критерия при конкретной функции потерь, необходимо аналитически найти оптимальное значение константы $c$ и подставить его в формулу для $H(R)$.\n",
    "\n",
    "Выведите критерий информативности для $L(y, c)=(y-c)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 14\n",
    "Какие деревья — глубокие или нет — следует брать в бэггинге? В бустинге? Ответ обосновать (из соображений разложения ошибки на шум, bias и variance)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
